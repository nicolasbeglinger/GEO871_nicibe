{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/nicibe/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/nicibe/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/nicibe/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/nicibe/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/nicibe/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/nicibe/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/nicibe/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download(\"maxent_ne_chunker\")\n",
    "nltk.download(\"words\")\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "import inspect\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prinfo(*args, **kwargs):\n",
    "    frame = inspect.currentframe().f_back\n",
    "    s = inspect.getframeinfo(frame).code_context[0]\n",
    "    r = re.search(r\"\\((.*)\\)\", s).group(1)\n",
    "    vnames = r.split(\", \")\n",
    "\n",
    "    if 'newline' in kwargs:\n",
    "        newlinestring = \"\\n\" if kwargs['newline'] else \"\"\n",
    "    else:\n",
    "        newlinestring = \"\"\n",
    "\n",
    "    for i, (var, val) in enumerate(zip(vnames, args)):\n",
    "        print(f\"{var} = {newlinestring}{val}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/texts/aid_wiki.txt\", \"r\") as wiki:\n",
    "    text = wiki.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = nltk.word_tokenize(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "filtered_list = [word for word in tokenized if word.casefold() not in stop_words and word[0].isalpha()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns = [word[0] for word in nltk.pos_tag(tokenized) if word[1] == 'NN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.PorterStemmer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed = [stemmer.stem(word) for word in nouns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized = [lemmatizer.lemmatize(word) for word in nouns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noun:               stem                 lemma:\n",
      "Aid                 aid                 Aid\n",
      "article             articl              article\n",
      "assistance          assist              assistance\n",
      "country             countri             country\n",
      "disambiguation      disambigu           disambiguation\n",
      "disambiguation      disambigu           disambiguation\n",
      "disambiguation      disambigu           disambiguation\n"
     ]
    }
   ],
   "source": [
    "print(f\"{'noun:': <20s}{'stem': <20s} lemma:\")\n",
    "for i, (noun, stem, lemma) in enumerate(zip(nouns, stemmed, lemmatized)):\n",
    "    print(f'{noun: <20s}{stem: <20s}{lemma}')\n",
    "    if i > 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Asia']\n"
     ]
    }
   ],
   "source": [
    "tokenized = nltk.word_tokenize(\"Asia\")\n",
    "tree = nltk.ne_chunk(nltk.pos_tag(tokenized))\n",
    "\n",
    "places = [\n",
    "    \" \".join(i[0] for i in t)\n",
    "    for t in tree\n",
    "    if hasattr(t, \"label\") and t.label() == \"GPE\"\n",
    "]\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "places = [lemmatizer.lemmatize(word) for word in places]\n",
    "print(sorted(places))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f27b1ac1f3c4d24f3bed65284a701a85f5d2e00d7c931fdc99d3fd228b36775"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('geo871': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
