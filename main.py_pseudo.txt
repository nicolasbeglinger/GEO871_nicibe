# %% [markdown]

# ## Import modules



# %%

IMPORT random

IMPORT inspect

IMPORT re

IMPORT os

IMPORT math





IMPORT pandas as pd

IMPORT geopandas as gpd

IMPORT folium

from folium IMPORT plugins

IMPORT nltk





IMPORT html2text



# %% [markdown]

# ## Minor Functions



# %% [markdown]

# Define function to debug



# %%





DEFINE FUNCTION prinfo(*args, **kwargs) -> None:

    """Prints arguments IN with the name beforehand:

    example:

    SET x TO 5

    OUTPUT(x)



    SET #OUTPUTs "x TO 5"

    """



    SET frame TO inspect.currentframe().f_back

    SET all_INPUT TO inspect.getframeinfo(frame).code_context[0]

    SET filtered_INPUT TO re.search(r"\((.*)\)", all_INPUT).group(1)

    SET split_INPUT TO filtered_INPUT.split(", ")



    IF 'newline' IN kwargs:

        SET newlinestring TO "\n" IF kwargs['newline'] else ""

    ELSE:

        SET newlinestring TO ""



    FOR var, val IN zip(split_INPUT, args):

        SET OUTPUT(f"{var} TO {newlinestring}{val}")



# %% [markdown]

# Define function to give information about the progress of reading the csv



# %%





DEFINE FUNCTION read_csv_in_chunks(path: str, n_lines: int, name_col: str, **read_params) -> pd.DataFrame:

    """Simmilar to pd.read_csv, but IN chunks that enable to see the progress



    Returns:

        [pandas.DataFrame]: [description]

    """



    IF 'chunksize' not IN read_params or read_params['chunksize'] < 1:

        SET read_params['chunksize'] TO 80000



    SET chunks TO [0] * math.ceil(n_lines / read_params['chunksize'])



    FOR chunk_idx, chunk IN enumerate(pd.read_csv(path, **read_params)):

        SET percent TO min(

            ((chunk_idx + 1) * read_params['chunksize'] / n_lines) * 100, 100.0)

        OUTPUT("#" * int(percent), f"{percent:.2f}%", end='\r', flush=True)

        SET chunks[chunk_idx] TO chunk[chunk[name_col].notnull()]



    OUTPUT()

    OUTPUT("Now concatenating chunks...")

    SET data_frame TO pd.concat(chunks, axis=0)

    del chunks

    OUTPUT("Finished!")

    RETURN data_frame



# %% [markdown]

# ## Import Stuff



# %% [markdown]

# ### Gazetteer



# %%





SET def get_gazetteer(gaz: str TO 'gns') -> dict:

    """Reads IN a gazetteer and RETURNs it as dataframe IN a property dictionary

    Args:

        gaz (str): ["nga", "geonames", "countries"]



    Returns:

        dict: {'df_gazetteer', 'idx_of_lat', 'idx_of_lon', 'nameCol'}

    """



    IF gaz EQUALS "gns":

        SET gazetteerpath TO "data/gazetteers/gns/countries_administrative.csv"

        SET # indexOfLat TO 4

        SET # indexOfLon TO 5

        SET idx_of_lat TO 1

        SET idx_of_lon TO 2

        SET n_lines TO 484618  # FOR gns administrative

        SET # n_lines TO  284485 #for gns administrative approved

        SET # n_lines TO 7866485 #for gns administrative populated

        SET name_col TO "SORT_NAME_RO"  # FOR gns



        # read IN the gazetter csv

        SET gazetteer TO read_csv_in_chunks(

            path=gazetteerpath,

            n_lines=n_lines,

            low_memory=False,

            name_col=name_col)



        SET gazetteer TO pd.concat(

            [gazetteer,

             pd.read_csv("data/gazetteers/own_places.csv")]

        )



    ELSEIF gaz EQUALS "geonames":

        SET gazetteerpath TO "data/geonames/allCountries_cleaned.csv"

        SET idx_of_lat TO 2

        SET idx_of_lon TO 3

        SET n_lines TO 6974472  # FOR allCountries_cleaned.csv

        SET # n_lines TO 2079830 # FOR allCountries_AT.csv

        SET name_col TO "name"



        # read IN the gazetter csv

        SET gazetteer TO read_csv_in_chunks(

            path=gazetteerpath,

            n_lines=n_lines,

            low_memory=False,

            name_col=name_col)



    # Read gazetter data (csv) and save placenames IN a list

    ELSEIF gaz EQUALS "countries":

        SET countrynames TO pd.read_csv(

            "data/geodict_github/countrynames.csv", names=["short", "long"])

        SET countrypositions TO pd.read_csv(

            "data/geodict_github/countrypositions.csv", names=["short", "lat", "lon"])



        SET gazetteer TO countrynames.merge(countrypositions, on="short")

        SET gazetteer.long TO gazetteer.long.str.strip()

        SET idx_of_lat TO 2

        SET idx_of_lon TO 3

        SET n_lines TO 240

        SET name_col TO "long"



    SET prop_dic TO {

        'df_gazetteer': gazetteer,

        'idx_of_lat': idx_of_lat,

        'idx_of_lon': idx_of_lon,

        'nameCol': name_col

    }



    RETURN prop_dic





# %% [markdown]

# ### Textfile



# %% [markdown]

# Import textfile or call the html2text function to extract the text from a given url \

# and save it FOR next use



# %%





SET def read_textfile(textfile: str TO None, url: str TO None) -> str:

    """Reads a textfile or extracts text from a website using html2text

    Args:

        textfile (str): path to textfile

        url (str): url of website



    Raises:

        Warning: If no path is specified



    Returns:

        str: loaded text or extracted text from website

    """



    IF textfile:

        with open(textfile, "r", encoding="utf8") as raw_text:

            SET text_str TO raw_text.read()



    ELSEIF url:

        SET text_save_path TO f"data/texts/autosave/lastText_{url[-20:]}"



        IF not os.path.exists(text_save_path):

            SET text_str TO html2text.html2text(url=url)



            with open(text_save_path, "w", encoding="utf8") as raw_text:

                raw_text.write(text_str)



        ELSE:

            with open(text_save_path, "r", encoding="utf8") as raw_text:

                SET text_str TO raw_text.read()



    ELSE:

        raise Warning("No path specified")



    RETURN text_str





SET # url TO 'https://www.theguardian.com/global-development/2021/dec/21/uk-accused-of-abandoning-\

# worlds-poor-as-aid-turned-into-colonial-investment'

SET # url TO "https://www.theguardian.com/world/2021/oct/21/cuts-to-overseas-aid-thwart-uk-efforts-\

# to-fight-covid-pandemic"



# %% [markdown]

# ## Matching



# %% [markdown]

# Extract places from text using nltk



# %%

DEFINE FUNCTION create_placenames(text_str: str, gazetteer: pd.DataFrame, prop_dic: dict) -> tuple:

    """Extract placenames out of a text and RETURN georeferenced results IN a geodataframe and dict

    Args:

        text_str (str): text that placenames should get extracted from

        gazetteer (pandas.Dataframe) gazetteer

        prop_dic (dict): Dictionary with keys /

            {'df_gazetteer', 'idx_of_lat', 'idx_of_lon', 'nameCol'}



    Returns:

        [type]: [description]

    """



    SET tokenized TO nltk.word_tokenize(text_str)

    SET tree TO nltk.ne_chunk(nltk.pos_tag(tokenized))



    SET i TO 0

    FOR word IN tokenized:

        IF word EQUALS "Asia":

            i += 1



    SET place_words TO [

        " ".join(i[0] FOR i IN t)

        FOR t IN tree

        IF hasattr(t, "label") and t.label() EQUALS "GPE"

    ]



    SET stemmer TO nltk.stem.PorterStemmer()

    SET lemmatizer TO nltk.stem.WordNetLemmatizer()

    SET place_words TO [lemmatizer.lemmatize(word).upper().replace(

        " ", "") FOR word IN place_words]

    # prinfo(sorted(places)[:10])

    OUTPUT()



    # Create Dictionary with placenames as keys and dictionaries with the counts as values.

    SET place_dic TO {}



    FOR place_word IN place_words:

        IF place_word IN place_dic:

            place_dic[place_word]["count"] += 1

        ELSE:

            SET place_dic[place_word] TO {"count": 1}



    # Fill the dictionary with the coordinates of the placenames.



    # add column with lemmatized placenames two compare them to lematized placenames of the text

    SET gazetteer["lemma_placenames"] TO gazetteer[prop_dic['nameCol']].apply(

        lemmatizer.lemmatize)

    SET gazetteer["stem_placenames"] TO gazetteer[prop_dic['nameCol']].apply(

        stemmer.stem)



    SET len_keys TO len(place_dic)

    SET log_interval TO 1  # int(round(lenKeys/20, 0))



    SET failed_places TO []



    FOR i, (placename, place_attributes) IN enumerate(place_dic.items()):

        TRY:

            SET tmp_df_values TO gazetteer.query(

                "lemma_placenames EQUALS @placename").values[0]

            SET # tmp_df_values TO df_gazetteer[df_gazetteer["SORT_NAME_RO"] EQUALS "MAFIKENG"].values[0]

            SET place_attributes["name"] TO tmp_df_values[7]

            SET place_attributes["lat"] TO tmp_df_values[prop_dic['idx_of_lat']]

            SET place_attributes["lon"] TO tmp_df_values[prop_dic['idx_of_lon']]



        except IndexError:

            failed_places.append(placename)



        # give feedback to progress

        IF (i % log_interval EQUALS 0 and i > 0) or i+1 EQUALS len_keys:

            OUTPUT(f"{i+1} of {len_keys} ({round((i/len_keys)*100, 1)}%)", end='\r')



    FOR i, (place_word, place_attributes) IN enumerate(place_dic.items()):

        IF i < 10:

            OUTPUT((place_word, place_attributes))



    # Catch information about nonfound placenames and delete them from the dictionary

    SET num_fails TO len(failed_places)

    FOR fail IN failed_places:

        del place_dic[fail]



    OUTPUT(f'{num_fails} words/places haven\'t been found IN the gazetteer')



    SET data_frame TO pd.DataFrame(

        [list(attributes.values()) + [place]

         FOR place, attributes IN place_dic.items()],

        columns=["count", "name", "lat", "lon", "stemname"])



    SET geo TO gpd.GeoDataFrame(

        data_frame,

        geometry=gpd.points_from_xy(data_frame.lon, data_frame.lat),

        crs=4326

    )



    RETURN (geo, place_dic)



    SET # geojsonname TO textfile[textfile.find("/")+1:textfile.find(".")][6:]

    # geo.to_file(f"data/geodataframes/{geojsonname}.geojson", driver='GeoJSON')



# %% [markdown]

# ## Initialization





# %%

SET dic TO read_gazetteer()

SET df_gazetteer TO dic['df_gazetteer']

SET text TO read_textfile(

    url="https://www.theguardian.com/global-development/2022/jan/14/worlds-poorest-bear-brunt-of-\

        climate-crisis-10-underreported-emergencies")

SET # text TO read_textfile("data/texts/tagesanzeiger_spendensammler.txt")

SET # text TO read_textfile("data/texts/aid_wiki.txt")

SET places, d TO create_placenames(

    text_str=text, gazetteer=df_gazetteer, prop_dic=dic)



# %% [markdown]

# ## Visualization



# %%

random.seed(1)



SET color_dic TO {}

FOR i IN range(places["count"].max()):

    SET color_dic[i+1] TO f"#{random.randint(0, 0xFFFFFF)}"

SET color_dic TO {

    1: '#440154',

    2: '#3b528b',

    3: '#21918c',

    4: '#5ec962',

    5: '#fde725'

}

OUTPUT(color_dic)



SET color_list TO [color_dic[count] FOR count IN places["count"]]



# %%

# create map

SET heatMap TO folium.Figure(width='75%')

SET heatMap TO folium.Map(

    location=[15, 30],

    zoom_start=2,

    max_bounds=True,

    tiles=None).add_to(heatMap)



# add tiles

folium.TileLayer(tiles='Cartodb dark_matter', name="Dark").add_to(heatMap)

folium.TileLayer(tiles='stamen watercolor', name="Watercolor").add_to(heatMap)



# add points and markercluster

SET points TO folium.FeatureGroup(name="Points", show=True).add_to(heatMap)

SET cluster TO plugins.MarkerCluster(

    name="Cluster", show=False).add_to(heatMap)



FOR place, attributes IN d.items():



    SET coordinates TO (attributes['lat'], attributes['lon'])



    SET html TO f'''



    <strong>Name:</strong> &emsp;&emsp;&emsp;&emsp;&emsp;{attributes['name']}<br/>

    <strong>Stemmed Name:</strong>&emsp;{place}<br/>

    <strong>Count:</strong>&emsp;&emsp;&emsp;&emsp;&emsp;{attributes['count']}

    

    '''



    SET iframe TO folium.IFrame(

        html,

        width=300,

        height=70)



    SET popup TO folium.Popup(iframe)



    folium.Circle(coordinates).add_to(cluster)

    folium.Circle(

        location=coordinates,

        popup=popup,

        tooltip=attributes['name'],

        radius=attributes['count'] * 50000,

        fill=True,

        color=color_dic[attributes['count']]

    ).add_to(points)





# extract coordinate of geodataframe

SET coordinates TO [[point.xy[1][0], point.xy[0][0]] FOR point IN places.geometry]



# add heatmap

plugins.HeatMap(

    name='HeatMap',

    data=coordinates,

    min_opacity=0.3,

    show=False

).add_to(heatMap)





SET new TO []

FOR place IN places.iterrows():

    FOR i IN range(place[1]['count']):

        new.append(place[1])



SET geo_multiple TO pd.DataFrame(new)



SET geo_multiple TO gpd.GeoDataFrame(

    geo_multiple,

    geometry=gpd.points_from_xy(geo_multiple.lon, geo_multiple.lat),

    crs=4326

)



# extract coordinate of geodataframe

SET coordinates TO [[point.xy[1][0], point.xy[0][0]]

               FOR point IN geo_multiple.geometry]



# add heatmap

plugins.HeatMap(

    name='HeatMap_multiple',

    data=coordinates,

    min_opacity=0.3,

    show=False

).add_to(heatMap)



# add layercontrol

folium.LayerControl(collapsed=False).add_to(heatMap)





heatMap



# %%

SET pointMap TO folium.Figure(width='35%')

gpd.GeoSeries.explore(

    places,

    color=color_list,

    max_bounds=True,

    tiles="Open Street Map",  # "Stamen Watercolor",

    marker_type='circle',

    marker_kwds={

        'radius': 50000,

        'fill': True}).add_to(pointMap)



# folium.TileLayer(tiles='stamen watercolor', name="Watercolor").add_to(pointMap)



pointMap



# %%

